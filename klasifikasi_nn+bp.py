# -*- coding: utf-8 -*-
"""Klasifikasi NN+BP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lBQvs7DGU6p72TAMR_WXISqFiYIOZI2-
"""

import numpy as np
import pandas as pd
from random import randrange
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import warnings
warnings.filterwarnings('ignore')

class NeuralNetworkBP():

  def __init__(self, hidden_layer_neurons, learning_rate, random_size, max_epoch):
    self.hidden_layer_neurons = hidden_layer_neurons
    self.learning_rate = learning_rate 
    self.random_size = random_size
    self.max_epoch = max_epoch
    pass

  def fit(self, X, y):  
    
    # learning_rate = 0.2 # slowly update the network
    error = []
    self.X = X
    self.y = y
    self.er = np.array([])
    self.w1 = 0
    self.w2 = 0

    num_inputs = len(self.X[0])
    num_outputs = len(self.y[0])
    np.random.seed(self.random_size)
    self.w1 = 2*np.random.random((num_inputs, self.hidden_layer_neurons)) - 1
    self.w2 = 2*np.random.random((self.hidden_layer_neurons, num_outputs)) - 1

    for epoch in range(self.max_epoch):
        # activate the first layer using the input
        #   matrix multiplication between the input and the layer 1 weights
        #   result is fed into a sigmoid function
        l1 = 1/(1 + np.exp(-(np.dot(self.X, self.w1))))
        # activate the second layer using first layer as input
        l2 = 1/(1 + np.exp(-(np.dot(l1, self.w2))))
        # find the average error of this batch
        #   using absolute, can use use square as well
        self.er = (abs(self.y - l2)).mean()
        error.append(self.er)
        
        # BACKPROPAGATION / learning!
        # find contribution of error on each weight on the second layer
        l2_delta = (self.y - l2)*(l2 * (1-l2))
        # update each weight in the second layer slowly
        self.w2 += l1.T.dot(l2_delta) * self.learning_rate
        
        # find contribution of error on each weight on the second layer w.r.t the first layer
        l1_delta = l2_delta.dot(self.w2.T) * (l1 * (1-l1))
        # udpate weights in the first layer
        self.w1 += X.T.dot(l1_delta) * self.learning_rate
    
    return self.w1, self.w2, self.er             
  
  def predict(self, X_test):
    self.X_test = X_test       
    
    l1 = 1/(1 + np.exp(-(np.dot(self.X_test, self.w1))))
    l2 = 1/(1 + np.exp(-(np.dot(l1, self.w2))))
      
    return l2

class cross_validaton():
  
  def __init__(self):
    pass

  # Split a dataset into k folds
  def cross_validation_split(self, dataset, folds):
    dataset_split = list()
    dataset_copy = list(dataset)
    fold_size = int(len(dataset) / folds)
    for i in range(folds):
      fold = list()
      while len(fold) < fold_size:
        index = randrange(len(dataset_copy))
        fold.append(dataset_copy.pop(index))
      dataset_split.append(fold)
    return dataset_split

  # test cross validation split
  # seed(10)
  # dataset = df_arr
  # folds = cross_validation_split(dataset, 4)
  # print(folds)

  def cross_validation_score(self, estimator, dataset, folds):
    self.estimator = estimator
    self.dataset = dataset
    self.folds = folds
    acuracy = np.array([])
    datafold = self.cross_validation_split(self.dataset, self.folds)
    datafold = np.asarray(datafold)

    print("K-Fold Cross-Validation")
    print("Dengan k = "+str(self.folds))
    print(" ")

    for i in range(self.folds):
      dataTest = np.array(datafold[i,:,:])
      dataTrain = np.delete(datafold, i, axis=0)
      dataTrain = np.array(dataTrain)
      dataTrain = dataTrain.reshape(-1,3)


      targets = [[1,0,0],[0,1,0],[0,0,1]]
      X_train = np.array(dataTrain[:, 0:2])
      y_train = np.array([targets[int(x)] for x in dataTrain[:, 2]])
      X_test = np.array(dataTest[:, 0:2])
      y_test = np.array([targets[int(x)] for x in dataTest[:, 2]])

      trainWeight = self.estimator.fit(X_train, y_train)
      predictY = self.estimator.predict(X_test)
      
      predictY = np.argmax(predictY, axis=1) # prediction
      y_test = np.argmax(y_test, axis=1) # prediction
      predictY = pd.DataFrame(predictY)
      y_test = pd.DataFrame(y_test)
      predictY = predictY.replace([0,1,2], ['AF','PVC','Normal'])
      y_test = y_test.replace([0,1,2], ['AF','PVC','Normal'])

      score = accuracy_score(y_test, predictY)
      acuracy = np.append(acuracy, score)

      print("Iterasi Ke-"+str(i+1)+" :")
      print(confusion_matrix(y_test, predictY))
      print(classification_report(y_test, predictY))


    print(acuracy)
    print("Mean Acuracy : ", acuracy.mean())

data = pd.read_csv('datasetFx1.csv')

dataset = data[['bpm', 'ibi', 'sdnn', 'sdsd', 'rmssd', 'pnn20', 'pnn50', 'hr_mad', 'sd1', 'sd2', 's', 'sd1/sd2', 'breathingrate', 'label']]
dataset.head(n=5)

df_norm = dataset[['bpm', 'ibi', 'sdnn', 'sdsd', 'rmssd', 'pnn20', 'pnn50', 'hr_mad', 'sd1', 'sd2', 's', 'sd1/sd2', 'breathingrate']].apply(lambda x: (x - x.min()) / (x.max() - x.min()))
df_norm[:5]

from sklearn.preprocessing import StandardScaler
# men-scaling dataset
scale_X = StandardScaler()
df_norm = scale_X.fit_transform(df_norm)
df_norm[:5]

from sklearn.decomposition import PCA
import plotly.express as px

pca = PCA()
pca.fit(df_norm)
exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)


plt.figure(figsize=(10,5))
plt.grid(True,which='both')
plt.plot(exp_var_cumul)
plt.xlim(0,7,1)
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')

pecea_data = PCA(n_components=2)
pecea_data.fit(df_norm)
pecea_dataframeX = pecea_data.transform(df_norm)
pecea_dataframeX[:5]

df_norm_pca = pd.DataFrame(pecea_dataframeX)
df_norm_pca[:5]

diagnosa = data[['label']].replace(['AF','PVC','Normal'],[0,1,2])
diagnosa[:5]

df = pd.concat([df_norm_pca, diagnosa], axis=1)
df[:5]

df_arr = np.array(df)
df_arr[:5]

df_arr.shape

NN = NeuralNetworkBP(hidden_layer_neurons=4, learning_rate=0.2, random_size=2, max_epoch=100)

kfoldCV = cross_validaton()

kfoldCV.cross_validation_score(NN, dataset=df_arr, folds=5)