# -*- coding: utf-8 -*-
"""Klasifikasi LVQ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/145VXoEHw7o2spH6Vdp_VtUzy9Zp3az8i
"""

import numpy as np
from random import seed 
from random import randrange
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, classification_report

import warnings
warnings.filterwarnings('ignore')

class lvqClassifier():

  def __init__(self, b, learn_rate, max_epoch):
    self.b = b
    self.max_epoch = max_epoch
    self.learn_rate = learn_rate 

    pass

  def lvq_fit(self, train, target):
    self.train = train
    self.target = target

    self.label, train_idx = np.unique(self.target, return_index=True)
    self.weight = self.train[train_idx].astype(np.float64)
    self.train = np.array([e for i, e in enumerate(zip(self.train, self.target)) if i not in train_idx])
    self.train, self.target = self.train[:,0], self.train[:,1]
    self.epoch = 0
    
    while self.epoch < self.max_epoch:
      for i, x in enumerate(self.train):
        distance = [sum((w-x)**2) for w in self.weight]
        min = np.argmin(distance)
        sign = 1 if self.target[1] == self.label[min] else -1
        # self.weight[min] = sign * self.learn_rate * (x - self.weight[min])
        self.weight[min] = self.weight[min] + (self.learn_rate * (x + (self.weight[min]*sign)))

      self.learn_rate *= self.b
      self.epoch += 1

    return self.weight, self.label

  def lvq_predict(self, test, w):
    predict = np.array([])  
    weight, label = w

    for i, x in enumerate(test):
      d = [sum((w - x) ** 2) for w in weight]
      
      diagnosis = label[np.argmin(d)]
      predict = np.append(predict, [diagnosis])

    return predict

class cross_validaton():
  
  def __init__(self):
    pass

  # Split a dataset into k folds
  def cross_validation_split(self, dataset, folds):
    dataset_split = list()
    dataset_copy = list(dataset)
    fold_size = int(len(dataset) / folds)
    for i in range(folds):
      fold = list()
      while len(fold) < fold_size:
        index = randrange(len(dataset_copy))
        fold.append(dataset_copy.pop(index))
      dataset_split.append(fold)
    return dataset_split

  # test cross validation split
  # seed(10)
  # dataset = df_arr
  # folds = cross_validation_split(dataset, 4)
  # print(folds)

  def cross_validation_score(self, estimator, dataset, folds):
    self.estimator = estimator
    self.dataset = dataset
    self.folds = folds
    acuracy = np.array([])
    datafold = self.cross_validation_split(self.dataset, self.folds)
    datafold = np.asarray(datafold)

    print("K-Fold Cross-Validation")
    print("Dengan k = "+str(self.folds))
    print(" ")

    for i in range(self.folds):
      dataTest = np.array(datafold[i,:,:])
      dataTrain = np.delete(datafold, i, axis=0)
      dataTrain = np.array(dataTrain)
      dataTrain = dataTrain.reshape(-1,3)

      X_train = np.array(dataTrain[:, 0:2])
      y_train = np.array(dataTrain[:, 2])
      X_test = np.array(dataTest[:, 0:2])
      y_test = np.array(dataTest[:, 2])

      trainWeight = self.estimator.lvq_fit(X_train, y_train)
      predictY = self.estimator.lvq_predict(X_test, trainWeight)
      
      score = accuracy_score(y_test, predictY)
      acuracy = np.append(acuracy, score)

      print("Iterasi Ke-"+str(i+1)+" :")
      print(confusion_matrix(y_test, predictY))
      print(classification_report(y_test, predictY))


    print(acuracy)
    print("Mean Acuracy : ", acuracy.mean())

import pandas as pd

data = pd.read_csv("datasetFx1.csv")
data.sample(n=10)

dataset = data[['bpm', 'ibi', 'sdnn', 'sdsd', 'rmssd', 'pnn20', 'pnn50', 'hr_mad', 'sd1', 'sd2', 's', 'sd1/sd2', 'breathingrate', 'label']]
dataset.head(n=5)

df_norm = dataset[['bpm', 'ibi', 'sdnn', 'sdsd', 'rmssd', 'pnn20', 'pnn50', 'hr_mad', 'sd1', 'sd2', 's', 'sd1/sd2', 'breathingrate']].apply(lambda x: (x - x.min()) / (x.max() - x.min()))
df_norm.sample(n=5)

from sklearn.preprocessing import StandardScaler
# men-scaling dataset
scale_X = StandardScaler()
df_norm = scale_X.fit_transform(df_norm)
df_norm[:5]

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import plotly.express as px

pca = PCA()
pca.fit(df_norm)
exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)

plt.figure(figsize=(10,5))
plt.grid(True,which='both')
plt.plot(exp_var_cumul)
plt.xlim(0,7,1)
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')

pecea_data = PCA(n_components=2)
pecea_data.fit(df_norm)
pecea_dataframeX = pecea_data.transform(df_norm)
pecea_dataframeX[:5]

df_norm_pca = pd.DataFrame(pecea_dataframeX)
df_norm_pca[:5]

diagnosa = data[['label']]
diagnosa.sample(n=5)

df = pd.concat([df_norm_pca, diagnosa], axis=1)
df.sample(n=5)

X = df.iloc[:,0:2]
y = df.iloc[:, 2]

X = np.array(X)
y = np.array(y)

print(X[:5])
print(y[:5])

df_arr = np.array(df)
df_arr[:5]

df_arr.shape

LVQ = lvqClassifier(b=.4, learn_rate=.01, max_epoch=10)

kfoldCV = cross_validaton()

kfoldCV.cross_validation_score(LVQ, dataset=df_arr, folds=5)

